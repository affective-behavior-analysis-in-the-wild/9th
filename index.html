<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>9th ABAW Workshop & Competition</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="vendor/aos/aos.css" rel="stylesheet">
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Gp
  * Updated: Nov 25 2023 with Bootstrap v5.3.2
  * Template URL: https://bootstrapmade.com/gp-free-multipurpose-html-bootstrap-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top ">
    <div class="container d-flex align-items-center justify-content-lg-between">

      <h1 class="logo me-auto me-lg-0"><a href="index.html">9th ABAW</a></h1>
      <!-- Uncomment below if you prefer to use an image logo -->
      <!-- <a href="index.html" class="logo me-auto me-lg-0"><img src="img/logo.png" alt="" class="img-fluid"></a>-->

      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <li><a class="nav-link scrollto" href="#about">About</a></li>
          <li><a class="nav-link scrollto" href="#team">Organisers</a></li>
          <li><a class="nav-link scrollto" href="#portfolio">Workshop</a></li>
            
          </li>
          
          <li class="dropdown"><a href="#clients"><span>Competition</span> <i class="bi bi-chevron-down"></i></a>
            <ul>
              <li><a href="#features">VA Estimation</a></li>
              <li><a href="#counts">CE Recognition</a></li>
              <li><a href="#counts2">Fine-Grained VD</a></li>
            </ul>
          </li>

        </ul>
      </nav><!-- .navbar -->

      <a href="https://cmt3.research.microsoft.com/9THABAW2025" class="get-started-btn scrollto">Submission Site</a>

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex align-items-center justify-content-center">
    <div class="container" data-aos="fade-up">

     <!-- <div class="row justify-content-center" data-aos="fade-up" data-aos-delay="150">   -->
     <!--   <div class="col-xl-6 col-lg-8">   -->
          <h1>9th Workshop and Competition on </h1> 
          <br> <h1> Affective & Behavior Analysis in-the-wild (ABAW)</h1>
          <h2>in conjunction with the International Conference on Computer Vision (ICCV), 2025 </h2>
          <h2> 8:00 - 12:30 HST, 19 October 2025, Honolulu, Hawaii, USA </h2>
        </div>
      </div>



    </div>
  </section><!-- End Hero -->






<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


  <main id="main">

    <!-- ======= About Section ======= -->
    <section id="about" class="about">
      <div class="container" data-aos="fade-up">

        <div class="row">
          <div class="col-lg-6 pt-4 pt-lg-0 order-2 order-lg-1 content" data-aos="fade-left" data-aos-delay="100">
            <img src="img/about.png" class="img-fluid" width="500" height="500" alt="">
          </div>
          <div class="col-lg-6 order-1 order-lg-2" data-aos="fade-right" data-aos-delay="100">
            <h3>About ABAW</h3>
            <p>
The ABAW Workshop is a premier platform highlighting the latest advancements in multimodal analysis, generation, modeling, and understanding of human affect and behavior in real-world, unconstrained environments. 
It emphasizes cutting-edge systems that integrate facial expressions, body movements, gestures, natural language, voice and speech to enable impactful research and practical applications. 
The workshop fosters interdisciplinary collaboration across fields such as computer vision, AI, human machine interaction, psychology, robotics, ethics & healthcare. 
The workshop further addresses complex challenges like algorithmic fairness, demographic bias & data privacy, making it a vital forum for building equitable, generalizable & human-centered AI systems. 
By uniting experts from academia, industry & government, the workshop promotes innovation, drives knowledge exchange, and inspires new directions in affective computing, behavior modelling and understanding & human-computer interaction. 
Finally, the Workshop includes a Competition with 3 challenges.              
</p>
            <p>
            The ABAW Workshop and Competition is a continuation of the respective events held at
<a href="https://affective-behavior-analysis-in-the-wild.github.io/8th/">CVPR 2025</a>, 
<a href="https://affective-behavior-analysis-in-the-wild.github.io/6th/">CVPR 2024</a>, 
<a href="https://ibug.doc.ic.ac.uk/resources/cvpr-2023-5th-abaw/">2023</a>, 
<a href="https://ibug.doc.ic.ac.uk/resources/cvpr-2022-3rd-abaw/">2022</a> & 
<a href="https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge/">2017</a>,
              
<a href="https://affective-behavior-analysis-in-the-wild.github.io/7th/">ECCV 2024</a> & 
<a href="https://ibug.doc.ic.ac.uk/resources/eccv-2023-4th-abaw/">2022</a>,
<a href="https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw/">ICCV 2021</a>,
<a href="https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affective-behavior-analysis/">FG 2020 (a)</a> & 
<a href="https://ibug.doc.ic.ac.uk/resources/affect-recognition-wild-unimulti-modal-analysis-va/">(b)</a>.
            </p>
          </div>
        </div>

      </div>
    </section><!-- End About Section -->






<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->

    <!-- ======= Team Section ======= -->
    <section id="team" class="team">
      <div class="container" data-aos="fade-up">

        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          <h2>Organisers</h2>
          
        </div>


<br></br>
        
        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          <h3>General Chair</h3>
          

                <br></br>

           </div>

         
          
        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">


      <div class="col-lg-3 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/jim.jpg" class="img-fluid" width="250" height="250" alt="">
                <div class="social">
                  <a href="https://sites.google.com/view/dimitrioskollias/home?authuser=0&pli=1"><i class="bi bi-list"></i></a>
                  <a href="https://scholar.google.com/citations?user=360Gmc0AAAAJ&hl=en"><i class="bi-journal-richtext"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Dimitrios Kollias   </h4>
                <span>Queen Mary University </span> <span> of London, UK </span> <span> d.kollias@qmul.ac.uk</span>
              </div>
            </div>
          </div>


              
       

    </div>

        <br></br>

         
        <div class="d-flex align-items-center justify-content-center" data-aos="fade-right" data-aos-delay="100">
          <h3>Program Chairs</h3>
          
        </div>
         
<br></br>

        
       <div class="row justify-content-center">



          <div class="col-lg col-md d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/stefanos.jpeg" class="img-fluid" width="250" height="250" alt="">
                <div class="social">
                  <a href="https://www.imperial.ac.uk/people/s.zafeiriou"><i class="bi bi-list"></i></a>
                  <a href="https://scholar.google.com/citations?user=QKOH5iYAAAAJ&hl=en&oi=ao"><i class="bi-journal-richtext"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Stefanos Zafeiriou   </h4>
                <span>Imperial College London, UK </span>    <span>  s.zafeiriou@imperial.ac.uk </span>
              </div>
            </div>
          </div>


     
          <div class="col-lg col-md d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/kotsia.jpg" class="img-fluid" width="250" height="250" alt="">
                <div class="social">
                  <a href="https://scholar.google.co.uk/citations?user=RwDV2kAAAAAJ&hl=en"></i><i class="bi-journal-richtext"></i></a>
                <a href="https://uk.linkedin.com/in/irene-kotsia-77690b5" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Irene Kotsia   </h4>
                <span>Cogitat Ltd, UK</span>
                <span>   irene@cogitat.io   </span>
              </div>
            </div>
          </div>     


          <div class="col-lg col-md d-flex align-items-stretch">
            <div class="member" data-aos="fade-up" data-aos-delay="100">
              <div class="member-img">
                <img src="img/greg.jpg" class="img-fluid" width="250" height="250" alt="">
                <div class="social">
                  <a href="https://scholar.google.com/citations?hl=en&user=oUK2gu8AAAAJ"><i class="bi-journal-richtext"></i></a>
                <a href="https://uk.linkedin.com/in/greg-slabaugh-a5b03a1" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                </div>
              </div>
              <div class="member-info">
                <h4>Greg Slabaugh   </h4>
                 <span>  Queen Mary University of London, UK     </span> 
                 <span>   g.slabaugh@qmul.ac.uk   </span>
              </div>
            </div>
          </div>

</div>
              


     <p></p>
         <div class="col-lg-6 order-1 order-lg-2" data-aos="fade-right" data-aos-delay="100">
            <h3>&nbsp &nbsp &nbsp &nbsp &nbsp Data Chairs</h3>
            
        </div> 
   
<p>
<h5>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Chunchang Shao,&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp   Queen Mary University of London, UK    </h5>
<h5>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Guanyu Hu,&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  Queen Mary University of London, UK & Xi'an Jiaotong University, China   </h5>
<h5>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Damith Chamalke Senadeera, &nbsp &nbsp  Queen Mary University of London, UK  </h5>
<h5>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Kaushal Kumar Keshlal Yadav,  &nbsp &nbsp  Queen Mary University of London, UK  </h5>
<h5>&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp Jianian Zheng,&nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  University College London, UK </h5>

</p>
       

</div>
      
    </section><!-- End Team Section -->





<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




    <!-- ======= Portfolio Section ======= -->
    <section id="portfolio" class="portfolio">
      <div class="container" data-aos="fade-up">


        <div class="d-flex align-items-center justify-content-center">
          <h1>The Workshop</h1>
        </div>

<br> </br>

        <div class="section-title">
          <h2>Call for Papers</h2>
          
        </div>

<div class="text">

 
<p>
Original high-quality contributions, in terms of databases, surveys, studies, foundation models, techniques and methodologies (either uni-modal or multi-modal; uni-task or multi-task ones) are solicited on -but are not limited to- the following topics: </p>

  <ul>
              <p><i class="ri-check-double-line"></i> facial expression (basic, compound or other) or micro-expression analysis</p>
              <p><i class="ri-check-double-line"></i>  facial action unit detection</p>
              <p><i class="ri-check-double-line"></i>  valence-arousal estimation</p>
              <p><i class="ri-check-double-line"></i>  physiological-based (e.g.,EEG, EDA) affect analysis</p>
              <p><i class="ri-check-double-line"></i>  face recognition, detection or tracking</p>
              <p><i class="ri-check-double-line"></i>  body recognition, detection or tracking</p>
              <p><i class="ri-check-double-line"></i>  gesture recognition or detection </p>
              <p><i class="ri-check-double-line"></i>  pose estimation or tracking </p>
              <p><i class="ri-check-double-line"></i>  activity recognition or tracking </p>
              <p><i class="ri-check-double-line"></i>  lip reading and voice understanding </p>
              <p><i class="ri-check-double-line"></i>  face and body characterization (e.g., behavioral understanding) </p>

              <p><i class="ri-check-double-line"></i>  characteristic analysis (e.g., gait, age, gender, ethnicity recognition) </p>

              <p><i class="ri-check-double-line"></i>  group understanding via social cues (e.g., kinship, non-blood relationships, personality) </p>
              <p><i class="ri-check-double-line"></i>  video, action and event understanding </p>
              <p><i class="ri-check-double-line"></i>  digital human modeling
 </p>
              <p><i class="ri-check-double-line"></i>  characteristic analysis (e.g., gait, age, gender, ethnicity recognition) </p>
              <p><i class="ri-check-double-line"></i>  violence detection </p>
              <p><i class="ri-check-double-line"></i>  autonomous driving </p>
              <p><i class="ri-check-double-line"></i>  domain adaptation, domain generalisation, few- or zero-shot learning for the above cases </p>
<p><i class="ri-check-double-line"></i>  fairness, explainability, interpretability, trustworthiness, privacy-awareness, bias mitigation and/or subgroup distribution shift analysis  for the above cases </p>
              <p><i class="ri-check-double-line"></i>  editing, manipulation, image-to-image translation, style mixing, interpolation, inversion and semantic diffusion for all afore mentioned cases    </p>
          
  </ul>
   

<br> </br>




        <div class="section-title">
          <h2>Workshop Important Dates </h2>
        </div>        


            <br>  <span> <em>Paper Submission Deadline: </em>    &nbsp &nbsp &nbsp   &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) July 8, 2025  </span> </br>
               <br>  <span>  <em>Review decisions sent to authors; Notification of acceptance: </em>  &nbsp &nbsp &nbsp  August  12, 2025 </span> </br> 
              <br>   <span>   <em>Camera ready version: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp August  18, 2025  </span> </br>
              
<br></br>
<br></br>
        <div class="section-title">
          <h2>Submission Information</h2>
          
        </div>        
              

<p>The paper format should adhere to the paper submission guidelines for main ICCV 2025 proceedings style. Please have a look at the Submission Guidelines Section <a href="https://iccv.thecvf.com/Conferences/2025/AuthorGuidelines">here</a>. </p>

<p>We welcome full long paper submissions (between 5 and 8 pages, excluding references or supplementary materials). All submissions must be anonymous and conform to the ICCV 2025 standards for double-blind review. </p>

<p>All papers should be submitted using <a href="https://cmt3.research.microsoft.com/9THABAW2025">this CMT website</a><sup><a href="#footnote1">*</a></sup>. </p>  

<p>All accepted manuscripts will be part of ICCV 2025 conference proceedings. </p>

<p>At the day of the workshop, oral presentations will be conducted by authors who are attending in-person.</p>

  <br>
  
<br>
<!-- Footnote section -->
<p id="footnote1"><strong>*</strong> The Microsoft CMT service was used for managing the peer-reviewing process for this workshop. This service was provided for free by Microsoft and they bore all expenses, including costs for Azure cloud services as well as for software development and support.</p>




  


  <br></br>
<br></br>
  
        <div class="section-title">
          <h2>Workshop Contact Information</h2>
        </div>  
  
 <p> For any queries you may have regarding the Workshop, please contact <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a>.
 </p>



            </div>
          </div>


    </section><!-- End Portfolio Section -->



<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




 




<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


    



    <!-- ======= Clients Section ======= -->
    <section id="clients" class="clients">
      <div class="container" data-aos="zoom-in">



        <div class="d-flex align-items-center justify-content-center">
          <h1>The Competition</h1>
        </div>


<br></br>

<p> The Competition is a continuation of the respective Competitions held at CVPR in 2025, 2024, 2023, 2022 & 2017, at ECCV in 2024 & 2022, at ICCV in 2021 and at IEEE FG in 2020. It is split into the three below mentioned Challenges. Participants are invited to participate in at least one of these Challenges.
 </p>


<br></br>




<h2>How to participate</h2>
 <p> In order to participate, teams will have to register. There is a maximum number of 8 participants in each team.
 </p>

<br></br>

<h3>VA Estimation Challenge</h3>
        

<p> If you want to participate <em>in this Challenge</em> you should follow the below procedure for registration.</p>

<p> The lead researcher should send  an email from their official address (no personal emails will be accepted) to <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a> with: </p>
<p>i) subject "9th ABAW Competition: Team Registration"; </p>
<p>ii) <a href = "https://drive.google.com/file/d/1StCT5NtE7acGSAYl2ki2gIBDcgwrx6uq/view?usp=sharing">this EULA</a> (if the team is composed of only academics) or 
<a href = "https://drive.google.com/file/d/153Gwos33iIMjJBlPifrtWIzz7ubzzETA/view?usp=sharing">this EULA</a> (if the team has at least one member coming from the industry) filled in, signed and attached; </p>  
<p>iii) the lead researcher's official academic/industrial website; the lead researcher cannot be a student (UG/PG/Ph.D.);</p>
<p>iv) the emails of each team member, each one in a separate line in the body of the email; </p>
<p>v) the team's name;</p>
<p>vi) the point of contact name and email address (which member of the team will be the main point of contact for future communications, data access etc) </p>


<p>As a reply, you will receive access to the dataset's cropped/cropped-aligned images and annotations and other important information.</p>

 
 <br></br>


<h3>CE Recognition Challenge </h3>
        
        

<p> If you want to participate <em>in this Challenge</em> you should follow the below procedure for registration.</p>

<p> The lead researcher should send  an email from their official address (no personal emails will be accepted) to <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a> with: </p>
<p>i) subject "9th ABAW Competition: Team Registration"; </p>
<p>ii) <a href = "https://drive.google.com/file/d/15wk_3GCjMfuxowGR8d0zSN5w5oHenS41/view?usp=sharing
">this EULA</a> (if the team is composed of only academics) or 
<a href = "https://drive.google.com/file/d/1te1rIKVns67NFNSWZs1pINm_-uywgOeX/view?usp=sharing
">this EULA</a> (if the team has at least one member coming from the industry) filled in, signed and attached; </p>  
<p>iii) the lead researcher's official academic/industrial website; the lead researcher cannot be a student (UG/PG/Ph.D.);</p>
<p>iv) the emails of each team member, each one in a separate line in the body of the email; </p>
<p>v) the team's name;</p>
<p>vi) the point of contact name and email address (which member of the team will be the main point of contact for future communications, data access etc) </p>


<p>As a reply, you will receive access to the dataset's videos and other important information.</p>

 
 <br></br>


 <h3>Fine-Grained VD Challenge </h3>
        
        

<p> If you want to participate <em>in this Challenge</em> you should follow the below procedure for registration.</p>

<p> The lead researcher should send  an email from their official address (no personal emails will be accepted) to <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a> with: </p>
<p>i) subject "9th ABAW Competition: Team Registration"; </p>
<p>ii) <a href = "https://drive.google.com/file/d/1-z7JH5HtsuFeIlT9a6t8QlNbMPkBAI2_/view?usp=sharing">this EULA</a> (if the team is composed of only academics) or 
<a href = "https://drive.google.com/file/d/14QTyCSMVsRzgDeKby0CUvPZuDTR_FOY9/view?usp=sharing">this EULA</a> (if the team has at least one member coming from the industry) filled in, signed and attached; </p>  
<p>iii) the lead researcher's official academic/industrial website; the lead researcher cannot be a student (UG/PG/Ph.D.);</p>
<p>iv) the emails of each team member, each one in a separate line in the body of the email; </p>
<p>v) the team's name;</p>
<p>vi) the point of contact name and email address (which member of the team will be the main point of contact for future communications, data access etc) </p>


<p>As a reply, you will receive access to the dataset's videos and other important information.</p>

 
 <br></br>       



<h2>Competition Contact Information</h2>

 <p> For any queries you may have regarding the Challenges, please contact <a href = "mailto: d.kollias@qmul.ac.uk">d.kollias@qmul.ac.uk</a>.
 </p>
      <br>  


        
<h2>General Information</h2> 
 <p> At the end of the Challenges, each team will have to send us:
 </p>

<p>i) a link to a Github repository where their solution/source code will be stored, </p>
<p>ii) a link to an ArXiv paper with 4-8 pages describing their proposed methodology, data used and results. </p>

<p>Each team will also need to upload their test set predictions on an evaluation server (details will be circulated when the test set is released).</p>

<p>After that, the winner of each Challenge, along with a leaderboard, will be announced.</p>

<p>There will be one winner per Challenge. The top-3 performing teams of each Challenge will have to contribute paper(s) describing their approach, methodology and results to our Workshop; the accepted papers will be part of the ICCV 2025 proceedings. All other teams are also able to submit paper(s) describing their solutions and final results; the accepted papers will be part of the ICCV 2025 proceedings.</p>

 

<p>The Competition's white paper (describing the Competition, the data, the baselines and results) will be ready at a later stage and will be distributed to the participating teams.</p>

 
 <br></br>
 

 <h2>General Rules</h2>

<p>1) Participants can contribute to any of the 3 Challenges.</p>

<p>2) In order to take part in any Challenge, participants will have to register as described above.</p>

<p>3) Any face detector whether commercial or academic can be used in the challenge. The paper accompanying the challenge result submission should contain clear details of the detectors/libraries used.</p>

<p>4) The top performing teams will have to share their solution (code, model weights, executables) with the organisers upon completion of the challenge; in this way the organisers will check so as to prevent cheating or violation of rules.</p>

 <br></br>
 
 <div class="section-title">
          <h2>Competition Important Dates</h2>
        </div>        

<br>  <span> <em>Call for participation announced, team registration begins, data available: </em>  &nbsp &nbsp &nbsp &nbsp &nbsp  June 3, 2025</span> </br>
      
<br>  <span> <em>Test set release: </em> &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp June 27, 2025</span> </br>

<br>  <span> <em>Final submission deadline (Predictions, Code and ArXiv paper): </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) July 3, 2025</span> </br>

<br>  <span> <em>Winners Announcement: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp July 5, 2025</span> </br>

              <br>  <span> <em>Final Paper Submission Deadline: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp 23:59:59 AoE (Anywhere on Earth) July 8, 2025  </span> </br>

               <br>  <span>  <em>Review decisions sent to authors; Notification of acceptance: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp  August 12, 2025 </span> </br> 
              <br>   <span>   <em>Camera ready version: </em> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp August 18, 2025  </span> </br>


 </div>

</section><!-- End Clients Section -->





<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->

    <!-- ======= Features Section ======= -->
    <section id="features" class="features">
      <div class="container" data-aos="fade-up">

        

  <div class="d-flex align-items-center justify-content-center">
          <h2>Valence-Arousal (VA) Estimation Challenge</h2>
        </div>


<h3>Database</h3> 
For this Challenge, an augmented version of the Aff-Wild2 database will be used. This database is audiovisual (A/V), in-the-wild and in total consists of 594 videos of around 3M frames of 584 subjects annotated in terms of valence and arousal.

 
<br> </br>

<h3>Rules</h3> 
Any solutions (either uni-task or multi-task) will be accepted for this Challenge.
Teams are allowed to use any -publicly or not- available pre-trained model (as long as it has not been pre-trained on Aff-Wild2). 
The pre-trained model can be pre-trained on any task (e.g., VA estimation, Expression Recognition, AU detection, Face Recognition). 
Teams are allowed to use other databases' annotations, or generated/synthetic data, or any affine transformations, or in general data augmentation techniques (e.g., <a href="https://openaccess.thecvf.com/content/CVPR2022W/ABAW/papers/Psaroudakis_MixAugment__Mixup_Augmentation_Methods_for_Facial_Expression_Recognition_CVPRW_2022_paper.pdf">MixAugment</a>) for increasing the size of the training dataset.   

 
<br> </br>
 
<h3>Performance Assessment</h3> 
<p>
The performance measure (P) is the mean Concordance Correlation Coefficient (CCC) of valence and arousal: 
<table>
       
         <td>
             <div class="num"> CCC<sub>arousal</sub> + CCC<sub>valence</sub> </div>
            <div class="denom">2</div>  
          </td>
        </tr>
        </table>
</p>




 
<h3>Baseline Results</h3> 

<p>The baseline network is a pre-trained on ImageNet ResNet-50 and its performance on the validation set is: </p>
<p>CCC<sub>valence</sub> = 0.24, &nbsp &nbsp  CCC<sub>arousal</sub> = 0.20 </p>
<p>P = 0.22 </p>





      </div>
    </section><!-- End Features Section -->



     

<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


    <!-- ======= Counts Section ======= -->
    <section id="counts" class="counts">
      <div class="container" data-aos="fade-up">

   

  <div class="d-flex align-items-center justify-content-center">
          <h2>Compound Expression (CE) Recognition Challenge</h2>
        </div>


<h3>Database</h3> 

For this Challenge, a part of C-EXPR-DB database will be used (56 videos in total). C-EXPR-DB is audiovisual (A/V) in-the-wild database and in total consists of 400 videos of around 200K frames; each frame is annotated in terms of 12 compound expressions. For this Challenge, the following 7 compound expressions will be considered: 
Fearfully Surprised, 
Happily Surprised, 
Sadly Surprised, 
Disgustedly Surprised, 
Angrily Surprised, 
Sadly Fearful and
Sadly Angry.
 <br></br>


<h3>Goal of the Challenge and Rules</h3> 
<p> Participants will be provided with a part of C-EXPR-DB database (56 videos in total), which will be unannotated, and will be required to develop their methodologies (supervised/self-supervised, domain adaptation, zero-/few-shot learning etc) for recognising the 7 compound expressions in this unannotated part, in a per-frame basis. </p>



<p>Teams are allowed to use any -publicly or not- available pre-trained model and any -publicly or not- available database (that contains any annotations, e.g. VA, basic or compound expressions, AUs) </p>


 
<h3>Performance Assessment</h3> 
<p>
The performance measure (P) is the average F1 Score across all 7 categories: &nbsp &sum; F1/7 </p>





      </div>
    </section><!-- End Counts Section -->


    <!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


    <!-- ======= Counts Section ======= -->
    <section id="counts2" class="counts">
      <div class="container" data-aos="fade-up">

   

  <div class="d-flex align-items-center justify-content-center">
          <h2>Fine-Grained Violence Detection (VD) Challenge</h2>
        </div>


<h3>Database</h3> 

For this Challenge, a part of DVD database will be used. 
DVD database is a large-scale (over 500 videos, 2.7M frames), frame-level annotated VD database with diverse environments, varying lighting conditions, multiple camera sources, complex social interactions, and rich metadata. 
DVD is designed to capture the complexities of real-world violent events.
 <br></br>


<h3>Goal of the Challenge and Rules</h3> 
<p> Participants will be provided with a subset of the DVD Database and will be tasked with developing AI, machine learning, or deep learning models for fine-grained violence detection (VD), specifically at the frame level. 
  Each frame in the DVD Database is annotated as either violent or non-violent. 
  Participants are required to predict, for every frame, whether it depicts a violent event (label: 1) or a non-violent event (label: 0). </p>



<p>Teams are allowed to use any -publicly or not- available pre-trained model and any -publicly or not- available database. </p>


 
<h3>Performance Assessment</h3> 
<p>
The performance measure (P) is the macro F1 Score across the two categories: &nbsp &sum; F1/2 </p>





      </div>
    </section><!-- End Counts Section -->




<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->




<!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


    <!-- ======= Cta Section ======= -->
    <section id="testimonials" class="testimonials">
      <div class="container" data-aos="zoom-in">


  <div class="d-flex align-items-center justify-content-center">
          <h2>References</h2>
        </div>

         </br>
<p> 
If you use the above data, you must cite all following papers: 
</p>
 

 <ul>
 <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition", 2025 </p> 
            <p><small>@article{Kollias2025, author = "Dimitrios Kollias and Panagiotis Tzirakis and Alan S. Cowen and Stefanos Zafeiriou and Irene Kotsia and Eric Granger and Marco Pedersoli and Simon L. Bacon and Alice Baird and Chris Gagne and Chunchang Shao and Guanyu Hu and Soufiane Belharbi and Muhammad Haseeb Aslam", title = "{Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition}", year = "2025", month = "3", url = "https://figshare.com/articles/preprint/CVPR_2025_ABAW8_baseline_paper_arxiv_pdf/28524563", doi = "10.6084/m9.figshare.28524563.v4"}</small></p>
            <p><small>@article{kolliasadvancements, title={Advancements in Affective and Behavior Analysis: The 8th ABAW Workshop and Competition}, author={Kollias, Dimitrios and Tzirakis, Panagiotis and Cowen, Alan and Kotsia, Irene and Cogitat, UK and Granger, Eric and Pedersoli, Marco and Bacon, Simon and Baird, Alice and Shao, Chunchang and others}}</small></p>

<p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "DVD: A Comprehensive Dataset for Advancing Violence Detection in Real-World Scenarios", 2025 </p> 
 <p><small>@misc{kollias2025dvd, author = {Kollias, Dimitrios and Senadeera, Damith and Zheng, Jianian and Yadav, Kaushal and Slabaugh, Greg and Awais, Muhammad and Yang, Xiaoyun}, title = {DVD: A Comprehensive Dataset for Advancing Violence Detection in Real-World Scenarios}, year = {2025}, howpublished = {\url{https://www.researchgate.net/publication/392397877_DVD_A_Comprehensive_Dataset_for_Advancing_Violence_D etection_in_Real-World_Scenarios}}, note = {DOI: \href{https://doi.org/10.13140/RG.2.2.31957.33762}{10.13140/RG.2.2.31957.33762}}}</small></p>


   
   <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "7th abaw competition: Multi-task learning and compound expression recognition", 2024 </p> 
            <p><small>@article{kollias20247th,title={7th abaw competition: Multi-task learning and compound expression recognition},author={Kollias, Dimitrios and Zafeiriou, Stefanos and Kotsia, Irene and Dhall, Abhinav and Ghosh, Shreya and Shao, Chunchang and Hu, Guanyu},journal={arXiv preprint arXiv:2407.03835},year={2024}}</small></p>

   
              <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition". IEEE CVPR, 2024  </p> 
            <p><small>@inproceedings{kollias20246th,title={The 6th affective behavior analysis in-the-wild (abaw) competition},author={Kollias, Dimitrios and Tzirakis, Panagiotis and Cowen, Alan and Zafeiriou, Stefanos and Kotsia, Irene and Baird, Alice and Gagne, Chris and Shao, Chunchang and Hu, Guanyu},booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},pages={4587--4598},year={2024}}</small></p>

              <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond". AAAI, 2024  </p> 
            <p><small>@inproceedings{kollias2024distribution,title={Distribution matching for multi-task learning of classification tasks: a large-scale study on faces \& beyond},author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos},booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},volume={38},number={3},pages={2813--2821},year={2024}}</small></p>

   
   <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges". IEEE CVPR, 2023 </p> 
            <p><small>@inproceedings{kollias2023abaw2, title={Abaw: Valence-arousal estimation, expression recognition, action unit detection \& emotional reaction intensity estimation challenges}, author={Kollias, Dimitrios and Tzirakis, Panagiotis and Baird, Alice and Cowen, Alan and Zafeiriou, Stefanos}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={5888--5897}, year={2023}} </small></p>

              <p><i class="ri-check-double-line"></i> D. Kollias: "Multi-Label Compound Expression Recognition: C-EXPR Database & Network". IEEE CVPR, 2023 </p> 
            <p><small>@inproceedings{kollias2023multi, title={Multi-Label Compound Expression Recognition: C-EXPR Database \& Network}, author={Kollias, Dimitrios}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={5589--5598}, year={2023}}</small></p>

              
              <p><i class="ri-check-double-line"></i> D. Kollias: "ABAW: Learning from Synthetic Data & Multi-Task Learning Challenges". ECCV, 2022 </p> 
            <p><small>@inproceedings{kollias2023abaw, title={ABAW: learning from synthetic data \& multi-task learning challenges}, author={Kollias, Dimitrios}, booktitle={European Conference on Computer Vision}, pages={157--172}, year={2023}, organization={Springer} } </small></p>


              <p><i class="ri-check-double-line"></i> D. Kollias: "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges". IEEE CVPR, 2022 </p> 
            <p><small>@inproceedings{kollias2022abaw, title={Abaw: Valence-arousal estimation, expression recognition, action unit detection \& multi-task learning challenges}, author={Kollias, Dimitrios}, booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages={2328--2336}, year={2022} }</small></p>



              <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Analysing Affective Behavior in the second ABAW2 Competition". ICCV, 2021 </p> 
            <p><small>@inproceedings{kollias2021analysing, title={Analysing affective behavior in the second abaw2 competition}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages={3652--3660}, year={2021}}</small></p>


              <p><i class="ri-check-double-line"></i> D. Kollias,S. Zafeiriou: "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework, 2021 </p> 
            <p><small>@article{kollias2021affect, title={Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2103.15792}, year={2021}}</small></p>

              <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study", 2021 </p> 
            <p><small>@article{kollias2021distribution, title={Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:2105.03790}, year={2021} }</small></p>




              <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Analysing Affective Behavior in the First ABAW 2020 Competition". IEEE FG, 2020 </p> 
            <p><small>@inproceedings{kollias2020analysing, title={Analysing Affective Behavior in the First ABAW 2020 Competition}, author={Kollias, D and Schulc, A and Hajiyev, E and Zafeiriou, S}, booktitle={2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)}, pages={794--800}}</small></p>



              <p><i class="ri-check-double-line"></i> D. Kollias, S. Zafeiriou: "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace". BMVC, 2019 </p> 
            <p><small>@article{kollias2019expression, title={Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace}, author={Kollias, Dimitrios and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:1910.04855}, year={2019}}</small></p>



              <p><i class="ri-check-double-line"></i> D. Kollias, et. al.: "Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge, Deep Architectures, and Beyond". International Journal of Computer Vision (IJCV), 2019 </p> 
            <p><small>@article{kollias2019deep, title={Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond}, author={Kollias, Dimitrios and Tzirakis, Panagiotis and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Schuller, Bj{\"o}rn and Kotsia, Irene and Zafeiriou, Stefanos}, journal={International Journal of Computer Vision}, pages={1--23}, year={2019}, publisher={Springer} }</small></p>


              <p><i class="ri-check-double-line"></i> D. Kollias, et at.: "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network", 2019 </p> 
            <p><small>@article{kollias2019face,title={Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network}, author={Kollias, Dimitrios and Sharmanska, Viktoriia and Zafeiriou, Stefanos}, journal={arXiv preprint arXiv:1910.11111}, year={2019}}</small></p>



              <p><i class="ri-check-double-line"></i> S. Zafeiriou, et. al. "Aff-Wild: Valence and Arousal in-the-wild Challenge". IEEE CVPR, 2017</p> 
            <p><small>@inproceedings{zafeiriou2017aff, title={Aff-wild: Valence and arousal ‘in-the-wild’challenge}, author={Zafeiriou, Stefanos and Kollias, Dimitrios and Nicolaou, Mihalis A and Papaioannou, Athanasios and Zhao, Guoying and Kotsia, Irene}, booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on}, pages={1980--1987}, year={2017}, organization={IEEE} } </small></p>






</ul>


 


 


 



 



   </div>
    </section><!-- End Counts Section -->









    <!-- ================================================================================================== -->
<!-- ================================================================================================== -->
<!-- ================================================================================================== -->


    <!-- ======= Cta Section ======= -->
    <section id="testimonials" class="testimonials">
      <div class="container" data-aos="zoom-in">


  <div class="d-flex align-items-center justify-content-center">
          <h2>Sponsors</h2>
        </div>

         </br>
<p> 
The Affective Behavior Analysis in-the-wild Workshop and Competition has been generously supported by:
 
</p>

         <ul>
              <p><i class="ri-check-double-line"></i>  Queen Mary University of London </p>
<p><small>  <img src="img/qmul.jpeg" alt="QMUL"> </small></p>

<p><i class="ri-check-double-line"></i>  Imperial College London </p>
<p><small>  <img src="img/icl.png" alt="ICL"> </small></p>
      
           
      
<p><i class="ri-check-double-line"></i>  Hume AI </p>
<p><small>  <img src="img/hume.jpeg" alt="HUME"> </small></p>
      
<p><i class="ri-check-double-line"></i>  École de technologie supérieure </p>
<p><small>  <img src="img/ets.png" alt="ETS"> </small></p>

           
<p><i class="ri-check-double-line"></i>  Concordia University </p>
<p><small>  <img src="img/concordia.png" alt="CON"> </small></p>
        

        



</ul>


 


 


 



 



   </div>
    </section><!-- End Counts Section -->






<!-- Comments go here 
ftiaxe faq


          <div class="col-lg-4 col-md-6 d-flex align-items-stretch mt-4" data-aos="zoom-in" data-aos-delay="300">
            <div class="icon-box">
              <div class="icon"><i class="bx bx-arch"></i></div>
              <h4><a href="">Divera don</a></h4>
              <p>Modi nostrum vel laborum. Porro fugit error sit minus sapiente sit aspernatur</p>
            </div>
          </div>

        </div>

      </div>
    </section>








    <section id="testimonials" class="testimonials">
      <div class="container" data-aos="zoom-in">

        <div class="testimonials-slider swiper" data-aos="fade-up" data-aos-delay="100">
          <div class="swiper-wrapper">

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-1.jpg" class="testimonial-img" alt="">
                <h3>Saul Goodman</h3>
                <h4>Ceo &amp; Founder</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Proin iaculis purus consequat sem cure digni ssim donec porttitora entum suscipit rhoncus. Accusantium quam, ultricies eget id, aliquam eget nibh et. Maecen aliquam, risus at semper.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-2.jpg" class="testimonial-img" alt="">
                <h3>Sara Wilsson</h3>
                <h4>Designer</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Export tempor illum tamen malis malis eram quae irure esse labore quem cillum quid cillum eram malis quorum velit fore eram velit sunt aliqua noster fugiat irure amet legam anim culpa.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-3.jpg" class="testimonial-img" alt="">
                <h3>Jena Karlis</h3>
                <h4>Store Owner</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Enim nisi quem export duis labore cillum quae magna enim sint quorum nulla quem veniam duis minim tempor labore quem eram duis noster aute amet eram fore quis sint minim.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-4.jpg" class="testimonial-img" alt="">
                <h3>Matt Brandon</h3>
                <h4>Freelancer</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Fugiat enim eram quae cillum dolore dolor amet nulla culpa multos export minim fugiat minim velit minim dolor enim duis veniam ipsum anim magna sunt elit fore quem dolore labore illum veniam.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>

            <div class="swiper-slide">
              <div class="testimonial-item">
                <img src="img/testimonials/testimonials-5.jpg" class="testimonial-img" alt="">
                <h3>John Larson</h3>
                <h4>Entrepreneur</h4>
                <p>
                  <i class="bx bxs-quote-alt-left quote-icon-left"></i>
                  Quis quorum aliqua sint quem legam fore sunt eram irure aliqua veniam tempor noster veniam enim culpa labore duis sunt culpa nulla illum cillum fugiat legam esse veniam culpa fore nisi cillum quid.
                  <i class="bx bxs-quote-alt-right quote-icon-right"></i>
                </p>
              </div>
            </div>
          </div>
          <div class="swiper-pagination"></div>
        </div>

      </div>
    </section>



    <section id="contact" class="contact">
      <div class="container" data-aos="fade-up">

        <div class="section-title">
          <h2>Contact</h2>
          <p>Contact Us</p>
        </div>

        <div>
          <iframe style="border:0; width: 100%; height: 270px;" src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d12097.433213460943!2d-74.0062269!3d40.7101282!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x0%3A0xb89d1fe6bc499443!2sDowntown+Conference+Center!5e0!3m2!1smk!2sbg!4v1539943755621" frameborder="0" allowfullscreen></iframe>
        </div>

        <div class="row mt-5">

          <div class="col-lg-4">
            <div class="info">
              <div class="address">
                <i class="bi bi-geo-alt"></i>
                <h4>Location:</h4>
                <p>A108 Adam Street, New York, NY 535022</p>
              </div>

              <div class="email">
                <i class="bi bi-envelope"></i>
                <h4>Email:</h4>
                <p>info@example.com</p>
              </div>

              <div class="phone">
                <i class="bi bi-phone"></i>
                <h4>Call:</h4>
                <p>+1 5589 55488 55s</p>
              </div>

            </div>

          </div>

          <div class="col-lg-8 mt-5 mt-lg-0">

            <form action="forms/contact.php" method="post" role="form" class="php-email-form">
              <div class="row">
                <div class="col-md-6 form-group">
                  <input type="text" name="name" class="form-control" id="name" placeholder="Your Name" required>
                </div>
                <div class="col-md-6 form-group mt-3 mt-md-0">
                  <input type="email" class="form-control" name="email" id="email" placeholder="Your Email" required>
                </div>
              </div>
              <div class="form-group mt-3">
                <input type="text" class="form-control" name="subject" id="subject" placeholder="Subject" required>
              </div>
              <div class="form-group mt-3">
                <textarea class="form-control" name="message" rows="5" placeholder="Message" required></textarea>
              </div>
              <div class="my-3">
                <div class="loading">Loading</div>
                <div class="error-message"></div>
                <div class="sent-message">Your message has been sent. Thank you!</div>
              </div>
              <div class="text-center"><button type="submit">Send Message</button></div>
            </form>

          </div>

        </div>

      </div>
    </section>
-->



  </main><!-- End #main -->


  <footer id="footer">
    <div class="footer-top">
      <div class="container">
        <div class="row">

   <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><span>Gp & Dimitrios Kollias</span></strong>. All Rights Reserved
      </div>
      <div class="credits">
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
      </div>
    </div>
  </footer>
  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

        </div>
      </div>
    </div>

 
  <!-- Vendor JS Files -->
  <script src="vendor/purecounter/purecounter_vanilla.js"></script>
  <script src="vendor/aos/aos.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="vendor/glightbox/js/glightbox.min.js"></script>
  <script src="vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="vendor/swiper/swiper-bundle.min.js"></script>
  <script src="vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="js/main.js"></script>

</body>

</html>
